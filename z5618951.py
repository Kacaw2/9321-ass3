import pandas as pd
import numpy as np
import sys
import warnings

warnings.filterwarnings("ignore")

from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import f1_score, mean_squared_error
from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingRegressor
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE

CATEGORICAL_COLS = ["category", "gender", "state", "job", "merchant", "city"]


def clean_and_prepare_data(df):
    """æ•°æ®æ¸…æ´—å’Œç‰¹å¾å·¥ç¨‹"""
    df_clean = df.copy()

    # 1. è½¬æ¢æ—¥æœŸåˆ—
    df_clean["trans_date_trans_time"] = pd.to_datetime(df_clean["trans_date_trans_time"])
    df_clean["dob"] = pd.to_datetime(df_clean["dob"])

    # 2. åˆ›å»ºå¹´é¾„ç‰¹å¾
    df_clean["age"] = (df_clean["trans_date_trans_time"] - df_clean["dob"]).dt.days / 365.25

    # 3. æ—¶é—´ç‰¹å¾
    df_clean["trans_hour"] = df_clean["trans_date_trans_time"].dt.hour
    df_clean["trans_dayofweek"] = df_clean["trans_date_trans_time"].dt.dayofweek
    df_clean["trans_month"] = df_clean["trans_date_trans_time"].dt.month
    df_clean["trans_day"] = df_clean["trans_date_trans_time"].dt.day
    df_clean["trans_quarter"] = df_clean["trans_date_trans_time"].dt.quarter
    # æ–°å¢ï¼šå‘¨æœ«å’Œå¤œé—´æ ‡è®°
    df_clean["is_weekend"] = df_clean["trans_dayofweek"].isin([5, 6]).astype(int)
    df_clean["is_night"] = ((df_clean["trans_hour"] >= 22) | (df_clean["trans_hour"] <= 6)).astype(int)
    
    # 4. åœ°ç†è·ç¦»ç‰¹å¾
    def haversine_np(lat1, lon1, lat2, lon2):
        R = 6371.0
        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
        dlat = lat2 - lat1
        dlon = lon2 - lon1
        a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2
        c = 2 * np.arcsin(np.sqrt(a))
        return R * c

    df_clean["customer_merchant_distance_km"] = haversine_np(
        df_clean["lat"], df_clean["long"], df_clean["merch_lat"], df_clean["merch_long"]
    )

    df_clean["distance_log"] = np.log1p(df_clean["customer_merchant_distance_km"])

    df_clean["is_long_distance"] = (df_clean["customer_merchant_distance_km"] > 100).astype(int)
    
    # 3. é‡‘é¢ç‰¹å¾
    df_clean["amt_log"] = np.log1p(df_clean["amt"])
    # é‡‘é¢ä¸è·ç¦»çš„äº¤äº’
    df_clean["amt_per_km"] = df_clean["amt"] / (df_clean["customer_merchant_distance_km"] + 1)
    
    df_clean["city_pop_log"] = np.log1p(df_clean["city_pop"])

    # åˆ é™¤ä¸éœ€è¦çš„åˆ—
    drop_cols = ["trans_date_trans_time", "dob", "unix_time", "cc_num", "first", "last", "street"]
    df_clean.drop(columns=[c for c in drop_cols if c in df_clean.columns], inplace=True)

    # è½¬æ¢åˆ†ç±»åˆ—
    for col in CATEGORICAL_COLS:
        if col in df_clean.columns:
            df_clean[col] = df_clean[col].astype("category")

    return df_clean


def encode_features(df, is_train=True, encoders=None):
    """ç‰¹å¾ç¼–ç """
    df_encoded = df.copy()

    if is_train:
        encoders = {}
        for col in CATEGORICAL_COLS:
            if col in df_encoded.columns:
                le = LabelEncoder()
                values = df_encoded[col].astype(str)
                df_encoded[f"{col}_encoded"] = le.fit_transform(values)
                encoders[col] = le
        df_encoded.drop(columns=CATEGORICAL_COLS, errors="ignore", inplace=True)
        return df_encoded, encoders
    else:
        if encoders is None:
            raise ValueError("When is_train=False, encoders must be provided.")
        for col in CATEGORICAL_COLS:
            if col in df_encoded.columns and col in encoders:
                le = encoders[col]
                mapping = {str(cls): idx for idx, cls in enumerate(le.classes_)}
                df_encoded[f"{col}_encoded"] = (
                    df_encoded[col].astype(str).map(mapping).fillna(-1).astype(int)
                )
        df_encoded.drop(columns=CATEGORICAL_COLS, errors="ignore", inplace=True)
        return df_encoded, encoders


def handle_class_imbalance(X_train, y_train, random_state=42):
    """ä½¿ç”¨SMOTEå¤„ç†ç±»åˆ«ä¸å¹³è¡¡"""
    print("\nå¤„ç†ç±»åˆ«ä¸å¹³è¡¡ (SMOTE)...")
    print(f"åŸå§‹è®­ç»ƒé›†å¤§å°: {len(X_train):,}")
    print(f"åŸå§‹æ¬ºè¯ˆæ ·æœ¬æ¯”ä¾‹: {y_train.mean()*100:.2f}%")
    
    smote = SMOTE(random_state=random_state, k_neighbors=5)
    X_balanced, y_balanced = smote.fit_resample(X_train, y_train)
    
    print(f"å¹³è¡¡åè®­ç»ƒé›†å¤§å°: {len(X_balanced):,}")
    print(f"å¹³è¡¡åæ¬ºè¯ˆæ ·æœ¬æ¯”ä¾‹: {y_balanced.mean()*100:.2f}%")
    
    return X_balanced, y_balanced


def main():
    
    if len(sys.argv) != 3:
      print("Usage: python3 z5618951.py <train_csv> <test_csv>")
      sys.exit(1)

    train_path = sys.argv[1]
    test_path = sys.argv[2]

    # è®­ç»ƒå›å½’æ¨¡å‹ï¼ˆç”¨äºéªŒè¯ï¼‰
    print("\nè®­ç»ƒå›å½’æ¨¡å‹ (ç”¨äºéªŒè¯)...")
    reg_params = {
        'max_iter': 200,
        'max_depth': 7,
        'learning_rate': 0.05,
        'l2_regularization': 0.1,
        'random_state': 42
    }
    
    model_reg = HistGradientBoostingRegressor(**reg_params)

    print(f"ä½¿ç”¨å‚æ•°: {reg_params}")
    print(f"ä½¿ç”¨æ¨¡å‹: {model_reg}")

    print("=" * 70)

    # è®­ç»ƒåˆ†ç±»æ¨¡å‹ï¼ˆç”¨äºéªŒè¯ï¼‰
    print("\nè®­ç»ƒåˆ†ç±»æ¨¡å‹ (ç”¨äºéªŒè¯)...")
    clf_params = {
        'n_estimators': 100,
        'max_depth': 20,
        'min_samples_split': 5,
        'class_weight': 'balanced_subsample',
        'random_state': 42,
        'n_jobs': -1
    }
    
    model_clf = RandomForestClassifier(**clf_params)
    
    print(f"ä½¿ç”¨å‚æ•°: {clf_params}")
    print(f"ä½¿ç”¨æ¨¡å‹: {model_clf}")

    print("=" * 70)
    print("Machine Learning Pipeline - Part II & Part III")
    print("=" * 70)

    # ============================================================
    # æ•°æ®åŠ è½½å’Œé¢„å¤„ç†ï¼ˆå¯¹ train å’Œ test åšå®Œå…¨ä¸€è‡´çš„å¤„ç†ï¼‰
    # ============================================================
    print("\nåŠ è½½æ•°æ®...")
    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)
    print(f"Training set: {len(train_df):,} rows")
    print(f"Test set: {len(test_df):,} rows")

    print("\næ¸…æ´—å’Œç‰¹å¾å·¥ç¨‹...")
    train_clean = clean_and_prepare_data(train_df)
    test_clean = clean_and_prepare_data(test_df)

    print("ç¼–ç ç‰¹å¾...")
    train_encoded, encoders = encode_features(train_clean, is_train=True)
    test_encoded, _ = encode_features(test_clean, is_train=False, encoders=encoders)

    # ============================================================
    # Part II - å›å½’ä»»åŠ¡ï¼ˆé¢„æµ‹äº¤æ˜“é‡‘é¢ amtï¼‰
    # ============================================================
    print("\n" + "=" * 70)
    print("Part II: Regression Task - Amount Prediction")
    print("=" * 70)

    # å‡†å¤‡å›å½’ç‰¹å¾ï¼ˆä¸åŒ…å« amtï¼Œå› ä¸ºè¿™æ˜¯ç›®æ ‡å˜é‡ï¼‰
    reg_feature_cols = [
        col for col in train_encoded.columns 
        if col not in ["trans_num", "is_fraud", "amt"]
    ]
    
    X_train_reg = train_encoded[reg_feature_cols]
    y_train_reg = train_encoded["amt"]
    X_test_reg = test_encoded[reg_feature_cols]

    print(f"\nç‰¹å¾æ•°é‡: {len(reg_feature_cols)}")
    print(f"è®­ç»ƒé›†å¤§å°: {len(X_train_reg):,}")
    print(f"æµ‹è¯•é›†å¤§å°: {len(X_test_reg):,}")
    print(f"äº¤æ˜“é‡‘é¢ç»Ÿè®¡:")
    print(f"  å‡å€¼: ${y_train_reg.mean():.2f}")
    print(f"  ä¸­ä½æ•°: ${y_train_reg.median():.2f}")
    print(f"  æ ‡å‡†å·®: ${y_train_reg.std():.2f}")

    
    model_reg.fit(X_train_reg, y_train_reg)
    print("âœ“ å›å½’æ¨¡å‹è®­ç»ƒå®Œæˆ")

    # ç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹
    print("\nç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹...")
    pred_reg_test = model_reg.predict(X_test_reg)
    print(f"âœ“ é¢„æµ‹å®Œæˆ: {len(pred_reg_test):,} ä¸ªæ ·æœ¬")
    print(f"é¢„æµ‹é‡‘é¢ç»Ÿè®¡:")
    print(f"  å‡å€¼: ${pred_reg_test.mean():.2f}")
    print(f"  ä¸­ä½æ•°: ${np.median(pred_reg_test):.2f}")

    # ç”Ÿæˆå›å½’è¾“å‡ºæ–‡ä»¶
    regression_output = pd.DataFrame({
        "trans_num": test_encoded["trans_num"],
        "amt": pred_reg_test
    })

    regression_output.to_csv("z5618951_regression.csv", index=False)
    print(f"\nâœ“ z5618951_regression.csv ({len(regression_output):,} è¡Œ)")

    print("\n" + "=" * 70)
    print("æµ‹è¯•é›†æ€§èƒ½è¯„ä¼° (å›å½’)")
    print("=" * 70)
    
    y_test_reg = test_encoded["amt"]
    rmse = np.sqrt(mean_squared_error(y_test_reg, pred_reg_test))
    
    print(f"Test RMSE: ${rmse:.2f}")
    print(f"Test é‡‘é¢ç»Ÿè®¡ (çœŸå®):")
    print(f"  å‡å€¼: ${y_test_reg.mean():.2f}")
    print(f"  ä¸­ä½æ•°: ${y_test_reg.median():.2f}")

    # ============================================================
    # Part III - åˆ†ç±»ä»»åŠ¡ï¼ˆæ£€æµ‹æ¬ºè¯ˆ is_fraudï¼‰
    # ============================================================
    print("\n" + "=" * 70)
    print("Part III: Classification Task - Fraud Detection")
    print("=" * 70)

    # å‡†å¤‡åˆ†ç±»ç‰¹å¾ï¼ˆåŒ…å« amtï¼‰
    clf_feature_cols = [
        col for col in train_encoded.columns if col not in ["trans_num", "is_fraud"]
    ]
    
    X_train_clf = train_encoded[clf_feature_cols]
    y_train_clf = train_encoded["is_fraud"]
    X_test_clf = test_encoded[clf_feature_cols]

    print(f"\nç‰¹å¾æ•°é‡: {len(clf_feature_cols)}")
    print(f"è®­ç»ƒé›†å¤§å°: {len(X_train_clf):,}")
    print(f"æµ‹è¯•é›†å¤§å°: {len(X_test_clf):,}")
    print(f"åŸå§‹æ¬ºè¯ˆæ ·æœ¬æ¯”ä¾‹: {y_train_clf.mean()*100:.2f}%")

    # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
    X_train_clf_balanced, y_train_clf_balanced = handle_class_imbalance(
        X_train_clf, y_train_clf, random_state=42
    )

    print("\nè®­ç»ƒåˆ†ç±»æ¨¡å‹...")

    model_clf.fit(X_train_clf_balanced, y_train_clf_balanced )
    print("âœ“ åˆ†ç±»æ¨¡å‹è®­ç»ƒå®Œæˆ")

    # éªŒè¯é›†è¯„ä¼°
    print("\nç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹...")
    pred_clf_test = model_clf.predict(X_test_clf)

    print(f"âœ“ é¢„æµ‹å®Œæˆ: {len(pred_clf_test):,} ä¸ªæ ·æœ¬")
    print(f"  é¢„æµ‹ä¸ºæ¬ºè¯ˆ: {pred_clf_test.sum():,} ({pred_clf_test.mean()*100:.2f}%)")
    print(f"  é¢„æµ‹ä¸ºæ­£å¸¸: {(pred_clf_test==0).sum():,} ({(pred_clf_test==0).mean()*100:.2f}%)")

    # ç”Ÿæˆåˆ†ç±»è¾“å‡ºæ–‡ä»¶
    classification_output = pd.DataFrame({
        "trans_num": test_encoded["trans_num"],
        "is_fraud": pred_clf_test
    })
    classification_output.to_csv("z5618951_classification.csv", index=False)
    print(f"\nâœ“ z5618951_classification.csv ({len(classification_output):,} è¡Œ)")

    print("\n" + "=" * 70)
    print("æµ‹è¯•é›†æ€§èƒ½è¯„ä¼°")
    print("=" * 70)

    y_test_clf = test_encoded["is_fraud"]
    f1_macro = f1_score(y_test_clf, pred_clf_test, average='macro')
    f1_weighted = f1_score(y_test_clf, pred_clf_test, average='weighted')
        
    print(f"Test F1 Score (Macro): {f1_macro:.4f}")
    print(f"Test F1 Score (Weighted): {f1_weighted:.4f}")
    print(f"Test æ¬ºè¯ˆæ ·æœ¬æ¯”ä¾‹ (çœŸå®): {y_test_clf.mean()*100:.2f}%")
        
    # é¢„ä¼°å¾—åˆ†
    if f1_macro >= 0.97:
            score = 5.0
            print(f"âœ“ Estimated score: {score:.2f}/5.0 ğŸ‰")
    elif f1_macro >= 0.85:
        score = ((f1_macro - 0.85) / 0.12) * 5
        print(f"âš  Estimated score: {score:.2f}/5.0")
    else:
        score = 0.0
        print(f"âœ— Estimated score: {score:.2f}/5.0 (F1 too low)")

    # ============================================================
    # æœ€ç»ˆæ€»ç»“
    # ============================================================
    print("\n" + "=" * 70)
    print("æœ€ç»ˆæ€»ç»“")
    print("=" * 70)

    print(f"Part II  - RMSE: {rmse:.2f}")

    print(f"Part III - F1 Score (Macro): {f1_macro:.4f}")

    print(f"Part III - Estimated Score: {score:.2f}/5.0")


    print("\nç”Ÿæˆçš„è¾“å‡ºæ–‡ä»¶:")
    print("  1. z5618951_regression.csv")
    print("  2. z5618951_classification.csv")
    print("=" * 70)
    print("\nâœ“ æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")


if __name__ == "__main__":
    main()
