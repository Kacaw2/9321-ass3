import pandas as pd
import numpy as np
import sys
import warnings

warnings.filterwarnings("ignore")

from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import f1_score, mean_squared_error
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE

CATEGORICAL_COLS = ["category", "gender", "state", "job", "merchant", "city"]


def clean_and_prepare_data(df):
    """æ•°æ®æ¸…æ´—å’Œç‰¹å¾å·¥ç¨‹"""
    df_clean = df.copy()

    # 1. è½¬æ¢æ—¥æœŸåˆ—
    df_clean["trans_date_trans_time"] = pd.to_datetime(df_clean["trans_date_trans_time"])
    df_clean["dob"] = pd.to_datetime(df_clean["dob"])

    # 2. åˆ›å»ºå¹´é¾„ç‰¹å¾
    df_clean["age"] = (df_clean["trans_date_trans_time"] - df_clean["dob"]).dt.days / 365.25

    # 3. æ—¶é—´ç‰¹å¾
    df_clean["trans_hour"] = df_clean["trans_date_trans_time"].dt.hour
    df_clean["trans_dayofweek"] = df_clean["trans_date_trans_time"].dt.dayofweek
    df_clean["trans_month"] = df_clean["trans_date_trans_time"].dt.month
    df_clean["trans_day"] = df_clean["trans_date_trans_time"].dt.day
    df_clean["trans_quarter"] = df_clean["trans_date_trans_time"].dt.quarter

    # 4. åœ°ç†è·ç¦»ç‰¹å¾
    def haversine_np(lat1, lon1, lat2, lon2):
        R = 6371.0
        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
        dlat = lat2 - lat1
        dlon = lon2 - lon1
        a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2
        c = 2 * np.arcsin(np.sqrt(a))
        return R * c

    df_clean["customer_merchant_distance_km"] = haversine_np(
        df_clean["lat"], df_clean["long"], df_clean["merch_lat"], df_clean["merch_long"]
    )

    df_clean["distance_log"] = np.log1p(df_clean["customer_merchant_distance_km"])
    df_clean["city_pop_log"] = np.log1p(df_clean["city_pop"])

    # åˆ é™¤ä¸éœ€è¦çš„åˆ—
    drop_cols = ["trans_date_trans_time", "dob", "unix_time", "cc_num", "first", "last", "street"]
    df_clean.drop(columns=[c for c in drop_cols if c in df_clean.columns], inplace=True)

    # è½¬æ¢åˆ†ç±»åˆ—
    for col in CATEGORICAL_COLS:
        if col in df_clean.columns:
            df_clean[col] = df_clean[col].astype("category")

    return df_clean


def encode_features(df, is_train=True, encoders=None):
    """ç‰¹å¾ç¼–ç """
    df_encoded = df.copy()

    if is_train:
        encoders = {}
        for col in CATEGORICAL_COLS:
            if col in df_encoded.columns:
                le = LabelEncoder()
                values = df_encoded[col].astype(str)
                df_encoded[f"{col}_encoded"] = le.fit_transform(values)
                encoders[col] = le
        df_encoded.drop(columns=CATEGORICAL_COLS, errors="ignore", inplace=True)
        return df_encoded, encoders
    else:
        if encoders is None:
            raise ValueError("When is_train=False, encoders must be provided.")
        for col in CATEGORICAL_COLS:
            if col in df_encoded.columns and col in encoders:
                le = encoders[col]
                mapping = {str(cls): idx for idx, cls in enumerate(le.classes_)}
                df_encoded[f"{col}_encoded"] = (
                    df_encoded[col].astype(str).map(mapping).fillna(-1).astype(int)
                )
        df_encoded.drop(columns=CATEGORICAL_COLS, errors="ignore", inplace=True)
        return df_encoded, encoders


def handle_class_imbalance(X_train, y_train, random_state=42):
    """ä½¿ç”¨SMOTEå¤„ç†ç±»åˆ«ä¸å¹³è¡¡"""
    print("\nå¤„ç†ç±»åˆ«ä¸å¹³è¡¡ (SMOTE)...")
    print(f"åŸå§‹è®­ç»ƒé›†å¤§å°: {len(X_train):,}")
    print(f"åŸå§‹æ¬ºè¯ˆæ ·æœ¬æ¯”ä¾‹: {y_train.mean()*100:.2f}%")
    
    smote = SMOTE(random_state=random_state, k_neighbors=5)
    X_balanced, y_balanced = smote.fit_resample(X_train, y_train)
    
    print(f"å¹³è¡¡åè®­ç»ƒé›†å¤§å°: {len(X_balanced):,}")
    print(f"å¹³è¡¡åæ¬ºè¯ˆæ ·æœ¬æ¯”ä¾‹: {y_balanced.mean()*100:.2f}%")
    
    return X_balanced, y_balanced


def main():
    if len(sys.argv) != 3:
        print("Usage: python3 z5618951.py <train_csv> <test_csv>")
        sys.exit(1)

    train_path = sys.argv[1]
    test_path = sys.argv[2]

    print("=" * 70)
    print("Machine Learning Pipeline - Part II & Part III")
    print("=" * 70)

    # ============================================================
    # æ•°æ®åŠ è½½å’Œé¢„å¤„ç†ï¼ˆå¯¹ train å’Œ test åšå®Œå…¨ä¸€è‡´çš„å¤„ç†ï¼‰
    # ============================================================
    print("\nåŠ è½½æ•°æ®...")
    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)
    print(f"Training set: {len(train_df):,} rows")
    print(f"Test set: {len(test_df):,} rows")

    print("\næ¸…æ´—å’Œç‰¹å¾å·¥ç¨‹...")
    train_clean = clean_and_prepare_data(train_df)
    test_clean = clean_and_prepare_data(test_df)

    print("ç¼–ç ç‰¹å¾...")
    train_encoded, encoders = encode_features(train_clean, is_train=True)
    test_encoded, _ = encode_features(test_clean, is_train=False, encoders=encoders)

    # ============================================================
    # Part II - å›å½’ä»»åŠ¡ï¼ˆé¢„æµ‹äº¤æ˜“é‡‘é¢ amtï¼‰
    # ============================================================
    print("\n" + "=" * 70)
    print("Part II: Regression Task - Amount Prediction")
    print("=" * 70)

    # å‡†å¤‡å›å½’ç‰¹å¾ï¼ˆä¸åŒ…å« amtï¼Œå› ä¸ºè¿™æ˜¯ç›®æ ‡å˜é‡ï¼‰
    reg_feature_cols = [
        col for col in train_encoded.columns 
        if col not in ["trans_num", "is_fraud", "amt"]
    ]
    
    X_train_reg = train_encoded[reg_feature_cols]
    y_train_reg = train_encoded["amt"]
    X_test_reg = test_encoded[reg_feature_cols]

    print(f"\nç‰¹å¾æ•°é‡: {len(reg_feature_cols)}")
    print(f"è®­ç»ƒé›†å¤§å°: {len(X_train_reg):,}")
    print(f"æµ‹è¯•é›†å¤§å°: {len(X_test_reg):,}")
    print(f"äº¤æ˜“é‡‘é¢ç»Ÿè®¡:")
    print(f"  å‡å€¼: ${y_train_reg.mean():.2f}")
    print(f"  ä¸­ä½æ•°: ${y_train_reg.median():.2f}")
    print(f"  æ ‡å‡†å·®: ${y_train_reg.std():.2f}")

    # åˆ’åˆ†éªŒè¯é›†ï¼ˆç”¨äºè¯„ä¼°ï¼‰
    X_train_reg_val, X_val_reg, y_train_reg_val, y_val_reg = train_test_split(
        X_train_reg, y_train_reg, test_size=0.15, random_state=42
    )
    print(f"\nè®­ç»ƒå­é›†: {len(X_train_reg_val):,} | éªŒè¯é›†: {len(X_val_reg):,}")

    # è®­ç»ƒå›å½’æ¨¡å‹ï¼ˆç”¨äºéªŒè¯ï¼‰
    print("\nè®­ç»ƒå›å½’æ¨¡å‹ (ç”¨äºéªŒè¯)...")
    reg_params = {
        'n_estimators': 150,
        'max_depth': 20,
        'min_samples_split': 5,
        'random_state': 42,
        'n_jobs': -1
    }
    print(f"ä½¿ç”¨å‚æ•°: {reg_params}")
    
    model_reg = RandomForestRegressor(**reg_params)
    model_reg.fit(X_train_reg_val, y_train_reg_val)
    print("âœ“ æ¨¡å‹è®­ç»ƒå®Œæˆ")

    # éªŒè¯é›†è¯„ä¼°
    print("\néªŒè¯é›†è¯„ä¼°:")
    pred_val_reg = model_reg.predict(X_val_reg)
    rmse = np.sqrt(mean_squared_error(y_val_reg, pred_val_reg))
    print(f"Validation RMSE: ${rmse:.2f}")

    # åœ¨å…¨é‡è®­ç»ƒæ•°æ®ä¸Šé‡æ–°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    print("\nåœ¨å…¨é‡è®­ç»ƒæ•°æ®ä¸Šé‡æ–°è®­ç»ƒ...")
    model_reg_final = RandomForestRegressor(**reg_params)
    model_reg_final.fit(X_train_reg, y_train_reg)
    print("âœ“ æœ€ç»ˆå›å½’æ¨¡å‹è®­ç»ƒå®Œæˆ")

    # ç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹
    print("\nç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹...")
    pred_reg_test = model_reg_final.predict(X_test_reg)
    print(f"âœ“ é¢„æµ‹å®Œæˆ: {len(pred_reg_test):,} ä¸ªæ ·æœ¬")
    print(f"é¢„æµ‹é‡‘é¢ç»Ÿè®¡:")
    print(f"  å‡å€¼: ${pred_reg_test.mean():.2f}")
    print(f"  ä¸­ä½æ•°: ${np.median(pred_reg_test):.2f}")

    # ç”Ÿæˆå›å½’è¾“å‡ºæ–‡ä»¶
    regression_output = pd.DataFrame({
        "trans_num": test_encoded["trans_num"],
        "amt": pred_reg_test
    })
    regression_output.to_csv("z5618951_regression.csv", index=False)
    print(f"\nâœ“ z5618951_regression.csv ({len(regression_output):,} è¡Œ)")

    # ============================================================
    # Part III - åˆ†ç±»ä»»åŠ¡ï¼ˆæ£€æµ‹æ¬ºè¯ˆ is_fraudï¼‰
    # ============================================================
    print("\n" + "=" * 70)
    print("Part III: Classification Task - Fraud Detection")
    print("=" * 70)

    # å‡†å¤‡åˆ†ç±»ç‰¹å¾ï¼ˆåŒ…å« amtï¼Œè§„å®šå…è®¸ï¼‰
    clf_feature_cols = [
        col for col in train_encoded.columns 
        if col not in ["trans_num", "is_fraud"]
    ]
    
    X_train_clf = train_encoded[clf_feature_cols]
    y_train_clf = train_encoded["is_fraud"]
    X_test_clf = test_encoded[clf_feature_cols]

    print(f"\nç‰¹å¾æ•°é‡: {len(clf_feature_cols)}")
    print(f"è®­ç»ƒé›†å¤§å°: {len(X_train_clf):,}")
    print(f"æµ‹è¯•é›†å¤§å°: {len(X_test_clf):,}")
    print(f"åŸå§‹æ¬ºè¯ˆæ ·æœ¬æ¯”ä¾‹: {y_train_clf.mean()*100:.2f}%")

    # åˆ’åˆ†éªŒè¯é›†
    print("\nåˆ’åˆ†éªŒè¯é›†...")
    X_train_val, X_val, y_train_val, y_val = train_test_split(
        X_train_clf, y_train_clf, test_size=0.15, random_state=42, stratify=y_train_clf
    )
    print(f"è®­ç»ƒå­é›†: {len(X_train_val):,} | éªŒè¯é›†: {len(X_val):,}")

    # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼ˆåªå¯¹è®­ç»ƒå­é›†ï¼‰
    X_train_val_balanced, y_train_val_balanced = handle_class_imbalance(
        X_train_val, y_train_val, random_state=42
    )

    # è®­ç»ƒåˆ†ç±»æ¨¡å‹ï¼ˆç”¨äºéªŒè¯ï¼‰
    print("\nè®­ç»ƒåˆ†ç±»æ¨¡å‹ (ç”¨äºéªŒè¯)...")
    clf_params = {
        'n_estimators': 200,
        'max_depth': 20,
        'min_samples_split': 2,
        'class_weight': 'balanced',
        'random_state': 42,
        'n_jobs': -1
    }
    print(f"ä½¿ç”¨å‚æ•°: {clf_params}")
    
    model_clf = RandomForestClassifier(**clf_params)
    model_clf.fit(X_train_val_balanced, y_train_val_balanced)
    print("âœ“ æ¨¡å‹è®­ç»ƒå®Œæˆ")

    # éªŒè¯é›†è¯„ä¼°
    print("\néªŒè¯é›†è¯„ä¼°:")
    pred_val = model_clf.predict(X_val)
    f1_macro = f1_score(y_val, pred_val, average='macro')
    f1_weighted = f1_score(y_val, pred_val, average='weighted')
    
    print(f"Validation F1 Score (Macro): {f1_macro:.4f}")
    print(f"Validation F1 Score (Weighted): {f1_weighted:.4f}")
    
    # é¢„ä¼°å¾—åˆ†
    if f1_macro >= 0.97:
        score = 5.0
        print(f"âœ“ Estimated score: {score:.2f}/5.0 ğŸ‰")
    elif f1_macro >= 0.85:
        score = ((f1_macro - 0.85) / 0.12) * 5
        print(f"âš  Estimated score: {score:.2f}/5.0")
    else:
        score = 0.0
        print(f"âœ— Estimated score: {score:.2f}/5.0 (F1 too low)")

    # åœ¨å…¨é‡è®­ç»ƒæ•°æ®ä¸Šé‡æ–°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    print("\nåœ¨å…¨é‡è®­ç»ƒæ•°æ®ä¸Šé‡æ–°è®­ç»ƒ...")
    X_train_clf_balanced, y_train_clf_balanced = handle_class_imbalance(
        X_train_clf, y_train_clf, random_state=42
    )
    
    model_clf_final = RandomForestClassifier(**clf_params)
    model_clf_final.fit(X_train_clf_balanced, y_train_clf_balanced)
    print("âœ“ æœ€ç»ˆåˆ†ç±»æ¨¡å‹è®­ç»ƒå®Œæˆ")

    # ç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹
    print("\nç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹...")
    pred_clf_test = model_clf_final.predict(X_test_clf)

    print(f"âœ“ é¢„æµ‹å®Œæˆ: {len(pred_clf_test):,} ä¸ªæ ·æœ¬")
    print(f"  é¢„æµ‹ä¸ºæ¬ºè¯ˆ: {pred_clf_test.sum():,} ({pred_clf_test.mean()*100:.2f}%)")
    print(f"  é¢„æµ‹ä¸ºæ­£å¸¸: {(pred_clf_test==0).sum():,} ({(pred_clf_test==0).mean()*100:.2f}%)")

    # ç”Ÿæˆåˆ†ç±»è¾“å‡ºæ–‡ä»¶
    classification_output = pd.DataFrame({
        "trans_num": test_encoded["trans_num"],
        "is_fraud": pred_clf_test
    })
    classification_output.to_csv("z5618951_classification.csv", index=False)
    print(f"\nâœ“ z5618951_classification.csv ({len(classification_output):,} è¡Œ)")

    # ============================================================
    # æœ€ç»ˆæ€»ç»“
    # ============================================================
    print("\n" + "=" * 70)
    print("æœ€ç»ˆæ€»ç»“")
    print("=" * 70)
    print(f"Part II  - RMSE: ${rmse:.2f}")
    print(f"Part III - F1 Score (Macro): {f1_macro:.4f}")
    print(f"Part III - Estimated Score: {score:.2f}/5.0")
    print("\nç”Ÿæˆçš„è¾“å‡ºæ–‡ä»¶:")
    print("  1. z5618951_regression.csv")
    print("  2. z5618951_classification.csv")
    print("=" * 70)
    print("\nâœ“ æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")


if __name__ == "__main__":
    main()
